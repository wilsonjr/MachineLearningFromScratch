{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset('penguins')\n",
    "penguins.dropna(inplace = True)\n",
    "X = np.array(penguins.drop(columns = ['species', 'island']))\n",
    "y = 1*np.array(penguins['species'] == 'Adelie')\n",
    "y[y == 0] = -1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions\n",
    "\n",
    "def get_weighted_pmk(y, weights):\n",
    "    ks = np.unique(y)\n",
    "    weighted_pmk = [sum(weights[y == k]) for k in ks]\n",
    "    \n",
    "    return np.array(weighted_pmk)/sum(weights)\n",
    "\n",
    "def gini_index(y, weights):\n",
    "    weighted_pmk = get_weighted_pmk(y, weights)\n",
    "    \n",
    "    return np.sum(weighted_pmk * (1-weighted_pmk))\n",
    "\n",
    "def cross_entropy(y, weights):\n",
    "    weighted_pmk = get_weighted_pmk(y, weights)\n",
    "    \n",
    "    return -np.sum(weighted_pmk * np.log2(weighted_pmk))\n",
    "\n",
    "def split_loss(child1, child2, weights1, weights2, loss=cross_entropy):\n",
    "    return (len(child1)*loss(child1, weights1) + len(child2)*loss(child2, weights2))/(len(child1) + len(child2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_rows_equal(X):\n",
    "    return (X == X[0]).all()\n",
    "\n",
    "def possible_splits(x):\n",
    "    L_values = []\n",
    "    for i in range(1, int(np.floor(len(x)/2)) + 1):\n",
    "        L_values.extend(list(combinations(x, i)))\n",
    "    \n",
    "    return L_values\n",
    "\n",
    "## Helper Classes\n",
    "class Node:\n",
    "    \n",
    "    def __init__(self, Xsub, ysub, observations, ID, depth = 0, parent_ID = None, leaf = True):\n",
    "        self.Xsub = Xsub\n",
    "        self.ysub = ysub\n",
    "        self.observations = observations\n",
    "        self.ID = ID\n",
    "        self.size = len(ysub)\n",
    "        self.depth = depth\n",
    "        self.parent_ID = parent_ID\n",
    "        self.leaf = leaf\n",
    "        \n",
    "\n",
    "class Splitter:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loss = np.inf\n",
    "        self.no_split = True\n",
    "        \n",
    "    def _replace_split(self, loss, d, dtype = 'quant', t = None, L_values = None):\n",
    "        self.loss = loss\n",
    "        self.d = d\n",
    "        self.dtype = dtype\n",
    "        self.t = t\n",
    "        self.L_values = L_values  \n",
    "        self.no_split = False\n",
    "\n",
    "        \n",
    "## Main Class\n",
    "class DecisionTreeClassifier:\n",
    "    \n",
    "    #############################\n",
    "    ######## 1. TRAINING ########\n",
    "    #############################\n",
    "    \n",
    "    ######### FIT ##########\n",
    "    def fit(self, X, y, weights, loss_func = cross_entropy, max_depth = 100, min_size = 2, C = None):\n",
    "        \n",
    "        ## Add data\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.N, self.D = self.X.shape\n",
    "        dtypes = [np.array(list(self.X[:,d])).dtype for d in range(self.D)]\n",
    "        self.dtypes = ['quant' if (dtype == float or dtype == int) else 'cat' for dtype in dtypes]\n",
    "        self.weights = weights\n",
    "        \n",
    "        ## Add model parameters\n",
    "        self.loss_func = loss_func\n",
    "        self.max_depth = max_depth\n",
    "        self.min_size = min_size\n",
    "        self.C = C\n",
    "        \n",
    "        ## Initialize nodes\n",
    "        self.nodes_dict = {}\n",
    "        self.current_ID = 0\n",
    "        initial_node = Node(Xsub = X, ysub = y, observations = np.arange(self.N), ID = self.current_ID, parent_ID = None)\n",
    "        self.nodes_dict[self.current_ID] = initial_node\n",
    "        self.current_ID += 1\n",
    "        \n",
    "        # Build\n",
    "        self._build()\n",
    "\n",
    "    ###### BUILD TREE ######\n",
    "    def _build(self):\n",
    "        \n",
    "        eligible_buds = self.nodes_dict \n",
    "        for layer in range(self.max_depth):\n",
    "            \n",
    "            ## Find eligible nodes for layer iteration\n",
    "            eligible_buds = {ID:node for (ID, node) in self.nodes_dict.items() if \n",
    "                                (node.leaf == True) &\n",
    "                                (node.size >= self.min_size) & \n",
    "                                (~all_rows_equal(node.Xsub)) &\n",
    "                                (len(np.unique(node.ysub)) > 1)}\n",
    "            if len(eligible_buds) == 0:\n",
    "                break\n",
    "            \n",
    "            ## split each eligible parent\n",
    "            for ID, bud in eligible_buds.items():\n",
    "                                \n",
    "                ## Find split\n",
    "                self._find_split(bud)\n",
    "                \n",
    "                ## Make split\n",
    "                if not self.splitter.no_split:\n",
    "                    self._make_split()\n",
    "                \n",
    "    ###### FIND SPLIT ######\n",
    "    def _find_split(self, bud):\n",
    "        \n",
    "        ## Instantiate splitter\n",
    "        splitter = Splitter()\n",
    "        splitter.bud_ID = bud.ID\n",
    "        \n",
    "        ## For each (eligible) predictor...\n",
    "        if self.C is None:\n",
    "            eligible_predictors = np.arange(self.D)\n",
    "        else:\n",
    "            eligible_predictors = np.random.choice(np.arange(self.D), self.C, replace = False)\n",
    "        for d in sorted(eligible_predictors):\n",
    "            Xsub_d = bud.Xsub[:,d]\n",
    "            dtype = self.dtypes[d]\n",
    "            if len(np.unique(Xsub_d)) == 1:\n",
    "                continue\n",
    "\n",
    "            ## For each value...\n",
    "            if dtype == 'quant':\n",
    "                for t in np.unique(Xsub_d)[:-1]:\n",
    "                    L_condition = Xsub_d <= t\n",
    "                    ysub_L = bud.ysub[L_condition]\n",
    "                    ysub_R = bud.ysub[~L_condition]\n",
    "                    weights_L = self.weights[bud.observations][L_condition]\n",
    "                    weights_R = self.weights[bud.observations][~L_condition]\n",
    "                    loss = split_loss(ysub_L, ysub_R,\n",
    "                                      weights_L, weights_R,\n",
    "                                      loss = self.loss_func)\n",
    "                    if loss < splitter.loss:\n",
    "                        splitter._replace_split(loss, d, 'quant', t = t)\n",
    "            else:\n",
    "                for L_values in possible_splits(np.unique(Xsub_d)):\n",
    "                    L_condition = np.isin(Xsub_d, L_values)\n",
    "                    ysub_L = bud.ysub[L_condition]\n",
    "                    ysub_R = bud.ysub[~L_condition]\n",
    "                    weights_L = self.weights[bud.observations][L_condition]\n",
    "                    weights_R = self.weights[bud.observations][~L_condition]\n",
    "                    loss = split_loss(ysub_L, ysub_R,\n",
    "                                      weights_L, weights_R,\n",
    "                                      loss = self.loss_func)\n",
    "                    if loss < splitter.loss: \n",
    "                        splitter._replace_split(loss, d, 'cat', L_values = L_values)\n",
    "                        \n",
    "        ## Save splitter\n",
    "        self.splitter = splitter\n",
    "    \n",
    "    ###### MAKE SPLIT ######\n",
    "    def _make_split(self):\n",
    "        \n",
    "        ## Update parent node\n",
    "        parent_node = self.nodes_dict[self.splitter.bud_ID]\n",
    "        parent_node.leaf = False\n",
    "        parent_node.child_L = self.current_ID\n",
    "        parent_node.child_R = self.current_ID + 1\n",
    "        parent_node.d = self.splitter.d\n",
    "        parent_node.dtype = self.splitter.dtype\n",
    "        parent_node.t = self.splitter.t        \n",
    "        parent_node.L_values = self.splitter.L_values\n",
    "        \n",
    "        ## Get X and y data for children\n",
    "        if parent_node.dtype == 'quant':\n",
    "            L_condition = parent_node.Xsub[:,parent_node.d] <= parent_node.t\n",
    "        else:\n",
    "            L_condition = np.isin(parent_node.Xsub[:,parent_node.d], parent_node.L_values)\n",
    "        Xchild_L = parent_node.Xsub[L_condition]\n",
    "        ychild_L = parent_node.ysub[L_condition]\n",
    "        child_observations_L = parent_node.observations[L_condition]\n",
    "        Xchild_R = parent_node.Xsub[~L_condition]\n",
    "        ychild_R = parent_node.ysub[~L_condition]\n",
    "        child_observations_R = parent_node.observations[~L_condition]\n",
    "        \n",
    "        ## Create child nodes\n",
    "        child_node_L = Node(Xchild_L, ychild_L, child_observations_L,\n",
    "                            ID = self.current_ID, depth = parent_node.depth + 1,\n",
    "                            parent_ID = parent_node.ID)\n",
    "        child_node_R = Node(Xchild_R, ychild_R, child_observations_R,\n",
    "                            ID = self.current_ID + 1, depth = parent_node.depth + 1,\n",
    "                            parent_ID = parent_node.ID)\n",
    "        self.nodes_dict[self.current_ID] = child_node_L\n",
    "        self.nodes_dict[self.current_ID + 1] = child_node_R\n",
    "        self.current_ID += 2\n",
    "                \n",
    "            \n",
    "    #############################\n",
    "    ####### 2. PREDICTING #######\n",
    "    #############################\n",
    "    \n",
    "    ###### LEAF MODES ######\n",
    "    def _get_leaf_modes(self):\n",
    "        self.leaf_modes = {}\n",
    "        for node_ID, node in self.nodes_dict.items():\n",
    "            if node.leaf:\n",
    "                values, counts = np.unique(node.ysub, return_counts=True)\n",
    "                self.leaf_modes[node_ID] = values[np.argmax(counts)]\n",
    "    \n",
    "    ####### PREDICT ########\n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        # Calculate leaf modes\n",
    "        self._get_leaf_modes()\n",
    "        \n",
    "        yhat = []\n",
    "        for x in X_test:\n",
    "            node = self.nodes_dict[0] \n",
    "            while not node.leaf:\n",
    "                if node.dtype == 'quant':\n",
    "                    if x[node.d] <= node.t:\n",
    "                        node = self.nodes_dict[node.child_L]\n",
    "                    else:\n",
    "                        node = self.nodes_dict[node.child_R]\n",
    "                else:\n",
    "                    if x[node.d] in node.L_values:\n",
    "                        node = self.nodes_dict[node.child_L]\n",
    "                    else:\n",
    "                        node = self.nodes_dict[node.child_R]\n",
    "            yhat.append(self.leaf_modes[node.ID])\n",
    "        return np.array(yhat)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    \n",
    "    def fit(self, X_train, y_train, T, stub_depth=1):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.N, self.D = X_train.shape\n",
    "        self.T = T\n",
    "        self.stub_depth = stub_depth\n",
    "        \n",
    "        self.weights = np.repeat(1/self.N, self.N)\n",
    "        self.trees = []\n",
    "        self.alphas = []\n",
    "        self.yhats = np.empty((self.N, self.T))\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            \n",
    "            self.T_t = DecisionTreeClassifier()\n",
    "            self.T_t.fit(self.X_train, self.y_train, self.weights, max_depth=self.stub_depth)\n",
    "            self.yhat_t = self.T_t.predict(self.X_train)\n",
    "            \n",
    "            self.epsilon_t = sum(self.weights*(self.yhat_t != self.y_train))/sum(self.weights)\n",
    "            self.alpha_t = np.log((1-self.epsilon_t)/self.epsilon_t)\n",
    "            \n",
    "            self.weights = np.array([w*(1-self.epsilon_t)/self.epsilon_t if self.yhat_t[i] != self.y_train[i]\n",
    "                                     else w for i, w in enumerate(self.weights)])\n",
    "            \n",
    "            self.trees.append(self.T_t)\n",
    "            self.alphas.append(self.alpha_t)\n",
    "            self.yhats[:, t] = self.yhat_t\n",
    "        \n",
    "        self.yhat = np.sign(np.dot(self.yhats, self.alphas))\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        yhats = np.zeros(len(X_test))\n",
    "        for t, tree in enumerate(self.trees):\n",
    "            yhats_tree = tree.predict(X_test)\n",
    "            yhats += yhats_tree*self.alphas[t]\n",
    "            \n",
    "        return np.sign(yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9761904761904762"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost = AdaBoost()\n",
    "boost.fit(X_train, y_train, T=30, stub_depth=3)\n",
    "\n",
    "yhat = boost.predict(X_test)\n",
    "np.mean(yhat==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
