{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From Friedman's book:\n",
    "\n",
    "This model allows us to assign each variable in our dataset a distribution, though by default they are all assumed to be Normal. Since each variable has its own distribution, estimating the model’s parameters is more involved. For each variable and each class, we estimate the parameters separately through the <strong>*_estimate_class_parameters*</strong>. The structure below allows for Normal, Bernoulli, and Poisson distributions, though any distribution could be implemented.\n",
    "\n",
    "Again, we make predictions by calculating $p(Y_n=k|x_n)$ for $k=1,…,K$ through Bayes’ rule and predicting the class with the highest posterior probability. Since each variable can have its own distribution, this problem is also more involved. The <strong>*_get_class_probability*</strong> method calculates the probability density of a test observation’s input variables. By the conditional independence assumption, this is just the product of the individual densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def _estimate_class_parameters(self, X_k):\n",
    "        \n",
    "        class_parameters = []\n",
    "        \n",
    "        for d in range(self.D):\n",
    "            X_kd = X_k[:, d] # only the dth column and the kth class\n",
    "            \n",
    "            if self.distributions[d] == 'normal':\n",
    "                mu = np.mean(X_kd)\n",
    "                sigma2 = np.var(X_kd)\n",
    "                class_parameters.append([mu, sigma2])\n",
    "                \n",
    "            if self.distributions[d] == 'bernoulli':\n",
    "                p = np.mean(X_kd)\n",
    "                class_parameters.append(p)\n",
    "            \n",
    "            if self.distributions[d] == 'poisson':\n",
    "                lam = np.mean(X_kd)\n",
    "                class_parameters.append(p)\n",
    "                \n",
    "        return class_parameters\n",
    "            \n",
    "        \n",
    "    def fit(self, X, y, distributions=None):\n",
    "        \n",
    "        self.N, self.D = X.shape\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        if distributions is None:\n",
    "            distributions = ['normal' for i in range(len(y))]\n",
    "        self.distributions = distributions\n",
    "        \n",
    "        # get prior probabilities\n",
    "        self.unique_y, unique_y_counts = np.unique(self.y, return_counts=True)\n",
    "        self.pi_ks = unique_y_counts/self.N\n",
    "        \n",
    "        # estimate parameters\n",
    "        self.parameters = []\n",
    "        for i, k in enumerate(self.unique_y):\n",
    "            X_k = self.X[self.y == k]\n",
    "            self.parameters.append(self._estimate_class_parameters(X_k))\n",
    "\n",
    "            \n",
    "    def _get_class_probability(self, x_n, j):\n",
    "        \n",
    "        class_parameters = self.parameters[j] # j is index of kth class\n",
    "        class_probability = 1\n",
    "        \n",
    "        for d in range(self.D):\n",
    "            x_nd = x_n[d]\n",
    "            \n",
    "            if self.distributions[d] == 'normal':\n",
    "                mu, sigma2 = class_parameters[d]\n",
    "                class_probability *= sigma2**(-1/2)*np.exp(-(x_nd - mu)**2/sigma2)\n",
    "            \n",
    "            if self.distributions[d] == 'bernoulli':\n",
    "                p = class_parameters[d]\n",
    "                class_probability *= (p**x_nd)*(1-p)**(1-x_nd)\n",
    "                \n",
    "            if self.distributions[d] == 'poisson':\n",
    "                lam = class_parameters[d]\n",
    "                class_probability *= np.exp(-lam)*lam**x_nd\n",
    "        \n",
    "        return class_probability\n",
    "    \n",
    "    def classify(self, X_test):\n",
    "        \n",
    "        y_n = np.zeros(len(X_test))\n",
    "        for i, x_n in enumerate(X_test):\n",
    "            \n",
    "            x_n = x_n.reshape(-1, 1)\n",
    "            p_ks = np.empty(len(self.unique_y))\n",
    "            \n",
    "            for j, k in enumerate(self.unique_y):\n",
    "                \n",
    "                p_x_given_y = self._get_class_probability(x_n, j)\n",
    "                p_y_given_x = self.pi_ks[j]*p_x_given_y\n",
    "                \n",
    "                p_ks[j] = p_y_given_x\n",
    "            \n",
    "            y_n[i] = self.unique_y[np.argmax(p_ks)]\n",
    "        \n",
    "        return y_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775280898876404"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(X, y)\n",
    "yhat = nb.classify(X)\n",
    "np.mean(yhat == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
